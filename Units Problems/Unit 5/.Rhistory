dtmRemoved = removeSparseTerms(dtmRemoved, 0.997)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = sparseRemoved
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
wordsRemoved
wikiWords = cbind(wordsAdded, wordsRemoved)
library(caTools)
set.seed(123)
wikiWords$Vandal = as.factor(wikiWords$Avg <= -1)
table(wikiWords$Vandal)
wikiWords$Vandal = wiki$Vandal
table(wikiWords$Vandal)
2061 / (2061+1815)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
split = sample.split(wiki$Vandal, SplitRatio = 0.7)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
train = subset(wikiWords, split=TRUE)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
train = subset(wikiWords, split==TRUE)
test = subset(wikiWords, split==FALSE)
library(rpart)
library(rpart.plot)
CART = rpart(Vandal ~. ,  data=wikiWords, method="class")
CART = rpart(Vandal ~. ,  data=train, method="class")
CART = rpart(Vandal ~ .,  data=train, method="class")
wikiCART = rpart(Vandal~. ,  data=train, method="class")
wikiCART = rpart(Vandal~. ,  data=train)
prp(wikiCART)
CART = rpart(train$Vandal ~ .,  data=train, method="class")
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
train = subset(wikiWords, split==TRUE)
test = subset(wikiWords, split==FALSE)
str(train)
train
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords
library(caTools)
set.seed(123)
wikiWords$Vandal = wiki$Vandal
str(wikiWords)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
train = subset(wikiWords, split==TRUE)
test = subset(wikiWords, split==FALSE)
table(wikiWords$Vandal)
train
train$Vandal
wikiCART = rpart(Vandal ~ .,  data=train, method="class")
wikiCART = rpart(Vandal~.,  data=train, method="class")
wikiCART = rpart('Vandal~.',  data=train, method="class")
split
wikiCART = rpart(wikiWords$Vandal ~ .,  data=train, method="class")
wikiCART = rpart(Vandal~., data=train, method="class")
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
train = subset(wikiWords, split==TRUE)
test = subset(wikiWords, split==FALSE)
library(rpart)
library(rpart.plot)
wikiCART <- rpart(Vandal~., data=train, method="class")
wikiCART <- rpart(Vandal~., data=wikiWords$train, method="class")
wikiCART <- rpart(Vandal~., data=train, method="class")
prp(wikiCART)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
str(wikiWords)
str(wikiWords2)
wikisplit = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
wikitrain = subset(wikiWords, wikisplit==TRUE)
wikitest = subset(wikiWords, split==FALSE)
wikiTrain = subset(wikiWords, wikisplit==TRUE)
wikiTest = subset(wikiWords, split==FALSE)
wikiCART = rpart(Vandal ~ ., data=wikiTrain, method="class")
testPredictCART = predict(wikiCART, newdata=wikiTest, type="class")
table(wikiTest$Vandal, testPredictCART)
grepl("cat","dogs and cats",fixed=TRUE) # TRUE
grepl("cat","dogs and rats",fixed=TRUE) # FALSE
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
wikiWords2$HTTP
table(wikiWords2$HTTP)
3659 + 217
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
wikiCART2 = rpart(Vandal~., data=wikiTrain2, method="class")
wikiCART2 = rpart(Vandal~., wikiTrain2, method="class")
str(wikiTrain2)
wikiWords = as.data.frame(as.matrix(wikiWords))
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
wikiCART2 = rpart(Vandal~., wikiTrain2, method="class")
wikiWords2 = wikiWords
spl = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
trials <- read.csv("clinical_trials.csv", stringsAsFactors=FALSE)
trials <- read.csv("clinical_trial.csv", stringsAsFactors=FALSE)
summary(trials)
str(trials)
nchar(trials)
nchar(trials$abstract)
which.max(nchar(trials$abstract))
nchar(trials$abstract)[664]
which(nchar(trials$abstract)==0)
(nchar(trials$abstract)==0)
table(nchar(trials$abstract)==0)
which.min(nchar(trials$title))
(nchar(trials$title))[1258]
nchar(trials$title)[1258]
trials$title[1258]
corpusTitle = Corpus(VectorSource(trials$title))
corpusAbstract = Corpus(VectorSource(trials$abstract))
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusAbstract = tm_map(corpusAbstract, removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
length(stopwords("english"))
dtmTitle
dtmAbstract
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmTitle
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmAbstract
colSums(trials$abstract)
colSums(dtmAbstract)
labeledAbstract = as.data.frame(as.matrix(dtmAbstract))
labeledAbstract
str(labeledAbstract)
table(labeledAbstract)
which.max(labeledAbstract)
colSums(labeledAbstract)
sort(colSums(labeledAbstract))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
dtmTitle
dtm = cbind(dtmTitle, dtmAbstract)
str(trials)
dtm$trial = trials$trial
str(dtm)
library(caTools)
set.seed(144)
trialspl = sample.split(dtm$trial, SplitRatio = 0.7)
trialtrain = subset(dtm, split==TRUE)
trialtest = subset(dtm, split==FALSE)
table(dtm$trial)
1043/(1043+817)
library(rpart)
library(rpart.plot)
trialCART= rpart(trial~., data=dtm, SplitRatio = 0.7)
trialCART= rpart(trial~., data=trialtrain, SplitRatio = 0.7)
trialCART= rpart("trial~.", data=trialtrain, SplitRatio = 0.7)
trialCART= rpart("trial~.", data=trials, SplitRatio = 0.7)
trialCART= rpart("trial~.", data=trials, method="class")
trialCART= rpart("trial~.", data=trialtrain, method="class")
trialspl = sample.split(dtm$trial, SplitRatio = 0.7)
trialspl = sample.split(trials$trial, SplitRatio = 0.7)
trialtrain = subset(trials, split==TRUE)
trialtest = subset(trials, split==FALSE)
trialCART= rpart("trial~.", data=trialtrain, method="class")
prp(trialCART)
trialCART
dtm
trialCART= rpart("trial~.", data=dtm, method="class")
trialCART= rpart("trial~.", data=trialtrain, method="class")
trialtrain$trial
trialtrain
dtm = cbind(dtmTitle, dtmAbstract)
trialsdf = as.data.frame(as.matrix(dtm))
trialspl = sample.split(trialdf$trial, SplitRatio = 0.7)
trialspl = sample.split(trialsdf$trial, SplitRatio = 0.7)
dtm = cbind(dtmTitle, dtmAbstract)
trial = as.data.frame(as.matrix(dtm))
trialspl = sample.split(trial$trial, SplitRatio = 0.7)
trialtrain = subset(trial, split==TRUE)
trialtest = subset(trial, split==FALSE)
trialCART= rpart("trial~.", data=trialtrain, method="class")
trialCART= rpart(trial~., data=trialtrain, method="class")
prp(trialCART)
trials <- read.csv("trials.csv", stringsAsFactors=FALSE)
str(trials)
trials <- read.csv("clinical_trial.csv", stringsAsFactors=FALSE)
str(trials)
library(tm)
library(SnowballC)
corpusTitle = Corpus(VectorSource(trials$title))
corpusAbstract = Corpus(VectorSource(trials$abstract))
# Convert to lower-case
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
# Remove punctuation
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
# Remove stopwords
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusAbstract = tm_map(corpusAbstract, removeWords, stopwords("english"))
# Stem document
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
# Create matrix
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmTitle
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmAbstract
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
dtmTitle
dtmAbstract
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmTitle
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmAbstract
# Remove sparse terms
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmTitle
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmAbstract
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
length(stopwords("english"))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
colnames(dtmTitle) = make.names(colnames(dtmTitle))
colnames(dtmAbstract) = make.names(colnames(dtmAbstract))
dtm = cbind(dtmTitle, dtmAbstract)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
dtm = cbind(dtmTitle, dtmAbstract)
Sparse$trial = trials$trial
dtm$trial = trials$trial
library(caTools)
set.seed(123)
trialsplit = sample.split(dtm$trial, SplitRatio = 0.7)
trialtrain = subset(dtm, split==TRUE)
testtrain = subset(dtm, split==FALSE)
library(rpart)
library(rpart.plot)
trialCART = rpart(trial ~ ., data=trialtrain, method="class")
prp(trialCART)
predictCART = predict(trialCART, newdata=trialtest, type="class")
table(dtm$trial, predictCART)
table(dtm$trial, predictCART > 0.5)
trialCART = rpart(trial ~ ., data=trialtrain, method="class")
prp(trialCART)
trialtest = subset(dtm, split==FALSE)
predictCART = predict(trialCART, newdata=trialtest, type="class")
table(dtm$trial, predictCART > 0.5)
table(dtm$trial, predictCART=0.5)
trialCART = rpart(trial ~ ., data=trialtrain, method="class")
prp(trialCART)
predictCART = predict(trialCART, newdata=trialtest, type="class")
table(dtm$trial, predictCART)
dtm$trial
predictCART
emails <- read.csv("emails.csv", stringsAsFactors=FALSE)
str(emails)
table(emails$spam)
corpus = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus= tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus
corpus[[1]]
corpus[[2]]
which.max(emails$text)
nchar(emails$text)
which.max(nchar(emails$text))
nchar(emails$text)[2651]
which.min(nchar(emails$text))
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
dtm = DocumentTermMatrix(corpus)
dtm
spdtm = removeSparseTerms(dtmTitle, 0.95)
spdtm = removeSparseTerms(dtm, 0.95)
spdtm
emailsSparse = as.data.frame(as.matrix(spdtm))
colnames(emailsSparse) = make.names(colnames(emailsSparse))
colSums(emailsSparse)
sort(colSums(emailsSparse))
emailsSparse$spam = emails$spam
colSums(emailsSparse)
sort(colSums(emailsSparse))
colSums(emailsSparse$ham)
colSums(emailsSparse$spam)
emailsSparse$spam
colSums(emailsSparse)
sort(colSums(emailsSparse))
sort(colSums(subset(emailsSparse, ham==0)))
sort(colSums(subset(emailsSparse, spam ==1)))
sort(colSums(subset(emailsSparse, spam==1)))
emailsSparse$spam = as.factor(emailsSparse$spam)
library(caTools)
set.seed(123)
spamsplit = sample.split(emailsSparse$spam, SplitRatio = 0.7)
spamtrain = subset(emailsSparse, split==TRUE)
spamtest = subset(emailsSparse, split==FALSE)
train = subset(emailsSparse, split==TRUE)
test = subset(emailsSparse, split==FALSE)
spamLog = glm(spam ~., data=emailsSparse)
spamLog = glm(spam ~., data=emailsSparse, family=binomial)
library(rpart)
library(rpart.plot)
spamLog = glm(spam ~., data=train, family=binomial)
spamCART = rpart(spam ~ ., data=train, method="class")
library(randomForest)
set.seed(123)
spamRF = randomForest(spam ~ ., data=train)
predictLog = predict(spamLog, newdata=train)
predictCART = predict(spamCART, newdata=train, type="class")
predictRF = predict(spamRF, newdata=train)
probCART = spamCART, newdata=train, type="class"
probCART = predict(spamCART, newdata=train)
probRF = predict(spamRF, newdata=train, type="prob")
probLog = predict(spamLog, newdata=train)
probLog
table(probLog <0.00001)
table(probLog >0.99999)
table(probLog >0.99999 & probLog <0.00001)
2991+1023
table(probLog<0.99999 & probLog>0.00001)
table(0.00001<probLog<0.99999)
spamLog
summary(spamLog)
summary(predictLog)
summary(spamLog)
prp(spamCART)
predictLog = predict(spamLog, newdata=train)
predictLog = predict(spamLog, newdata=train, threshold=0.5)
table(predictLog)
table(predictLog$spam)
predictLog = predict(spamLog, newdata=train)
table(emailsSparse$spam, predictLog >0.5)
table(train$spam, predictLog >0.5)
(933+2975)/(2975+933+16+90)
table(test$spam, predictLog >0.5)
library(ROCR)
predLog = predict(spamLog, newdata=train)
probLog = predLog[,2]
predCART = predict(spamCART, newdata=train)
probCART = predCART[,2]
predRF = predict(spamRF, newdata=train, type="prob")
probRF = predRF[,2]
predLog
probLog = predLog[,1]
summary(predLog)
table(test$spam, predLog >0.5)
table(train$spam, predictLog >0.5)
table(train$spam, predLog >0.5)
table(train$spam, predLog)
table(train$spam, predLog >0.5)
table(train$spam)
3065 / (949 + 3065)
predROCR = prediction(predLog, train$spam)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
# Compute AUC
performance(predROCR, "auc")@y.values
(2975+933)/(2975+933+90+16)
(3052+954)/nrow(train)
table(train$spam, probCART >0.5)
(2919+889) / nrow(train)
predROCR = prediction(probCART, train$spam)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
performance(predROCR, "auc")@y.values
table(train$spam, probRF >0.5)
(3058+949) / nrow(train)
predROCR = prediction(probRF, train$spam)
perfROCR = performance(predROCR, "tpr", "fpr")
performance(predROCR, "auc")@y.values
plot(perfROCR, colorize=TRUE)
predLog2 = predict(spamLog, newdata=test)
predCART2 = predict(spamCART, newdata=test)
probCART2 = predCART2[,2]
predRF2 = predict(spamRF, newdata=test, type="prob")
probRF2 = predRF2[,2]
table(test$spam, predLog2 >0.5)
table(test$spam, predLog2)
table(test$spam, predLog2 >0.5)
(1228+394)/nrow(test)
predROCR = prediction(predLog2, train$spam)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
performance(predROCR, "auc")@y.values
predROCR = prediction(predLog2, test$spam)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
performance(predROCR, "auc")@y.values
table(test$spam, probCART2 >0.5)
(1219+380)/nrow(test)
predROCR = prediction(probCART2, test$spam)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
performance(predROCR, "auc")@y.values
table(test$spam, probRF2 >0.5)
(1276+397)/nrow(test)
predROCR = prediction(probRF2, test$spam)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
performance(predROCR, "auc")@y.values
wiki <- read.csv("wiki.csv", stringsAsFactors=FALSE)
str(wiki)
table(wiki$Vandal)
wiki$Vandal = as.factor(wiki$Vandal)
library(tm)
library(SnowballC)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, tolower)
corpusAdded = tm_map(corpusAdded, PlainTextDocument)
corpusAdded = tm_map(corpusAdded, removePunctuation)
corpusAdded = tm_map(corpusAdded, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, tolower)
corpusRemoved = tm_map(corpusRemoved, PlainTextDocument)
corpusRemoved = tm_map(corpusRemoved, removePunctuation)
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english")
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
dtmRemoved
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english")
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
dtmRemoved
length(stopwords("english"))
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
sparseRemoved
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("A", colnames(wordsRemoved))
colnames(wordsAdded) = make.names(colnames(wordsAdded))
colnames(wordsRemoved) = make.names(colnames(wordsRemoved))
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
wikiWords$Vandal = as.factor(wikiWords$Vandal)
library(caTools)
set.seed(123)
vansplit = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
vantrain = subset(wikiWords, split==TRUE)
vantest = subset(wikiWords, split==FALSE)
vanCART = rpart(Vandal ~ ., data=vantrain, method="class")
prp(vanCART)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
vantrain = subset(wikiWords, split==TRUE)
vantest = subset(wikiWords, split==FALSE)
vanCART = rpart(Vandal ~ ., data=vantest, method="class")
prp(vanCART)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
wikiCART2 = rpart(Vandal ~ ., data=wikiTrain2, method="class")
prp(wikiCART2)
table(wikiCART2)
predCART2 <- predict(wikiCART2, newdata=wikiTest2, type="class")
table(wikiCART2, predCART2>0.5)
table(wikiTest2$Vandal, predCART2>=0.5)
table(wikiTest2$Vandal)
615 /(615+545)
table(wikiTest2$Vandal, predCART2>0.5)
predCART2 <- predict(wikiCART2, newdata=wikiTest2)
pred.prob = predCART2[,2]
table(wikiTest2$Vandal, pred.prob>0.5)
(596+57)/nrow(test)
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
summary(wikiWords2$NumWordsAdded)
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
wikiCART3 = rpart(Vandal ~ ., data=wikiTrain2, method="class")
prp(wikiCART3)
predictCART3 = predict(wikiCART3, newdata=wikiTest2, type="class")
probCART3 = predict(wikiCART3, newdata=wikiTest2)
probCART3 = predict(wikiCART3, newdata=wikiTest2)[,2]
table(wikiTest2$Vandal, probCART3 >0.5)
(498+244)/nrow(wikiTest2)
(498+244)/(498+244+301+117)
table(wikiTest2$Vandal)
table(wikiTest2$Vandal, predictCART3 >0.5)
table(wikiTest2$Vandal, predictCART3 >=0.5)
probCART3 = predict(wikiCART3, newdata=wikiTrain2)[,2]
table(wikiTest2$Vandal, probCART3 >0.5)
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
wikiTest3 = subset(wikiWords3, spl==FALSE)
wikiTrain3 = subset(wikiWords3, spl==TRUE)
wikiCART3 = rpart(Vandal ~ ., data=wikiTrain3, method="class")
prp(wikiCART3)
