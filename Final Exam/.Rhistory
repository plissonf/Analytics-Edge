results <- boot(data=Xy, statistic=bs,
R=1000)
bs <- function(formula, data, indices) {
d <- data[indices,] # allows boot to select sample
fit <- lm(formula, data=d)
return(coef(fit))
}
results <- boot(data=Xy, statistic=bs,
R=1000)
lm.fn <- function(formula,data, indices)
{
fit <- lm(formula, data=d)
coeff <- coef(fit)
#sd <- sd(beta1)
return(coeff)
}
results <- boot(Xy, lm.fn, R=1000,formula=y~X1+X2)
results
lm.fn <- function(formula,data, indices){
fit <- lm(y~X1+X2, data=d)
coeff <- coef(fit)
return(coeff)
}
results <- boot(data=Xy, statistic=lm.fn, R=1000)
results
results <- boot(data=Xy, statistic=lm.fn, R=1000)
library(boot)
lm.fn <- function(formula,data, indices){
fit <- lm(y~X1+X2, data=Xy)
coeff <- coef(fit)
return(coeff)
}
results <- boot(data=Xy, statistic=lm.fn, R=1000)
results
lm.fn <- function(formula, data, indices){
fit <- lm(formula,data)
coeff <- coef(fit)
return(coeff)
}
results <- boot(data=Xy, statistic=lm.fn, R=1000, formula=y~X1+X2)
results
lm.fn <- function(formula, data, indices){
d <- data[indices,]
fit <- lm(formula,data=d)
coeff <- coef(fit)
return(coeff)
}
results <- boot(data=Xy, statistic=lm.fn, R=1000, formula=y~X1+X2)
results
library(boot)
lm.fn <- function(formula, data, indices){
d <- data[indices,]
fit <- lm(formula,data=d)
coeff <- coef(fit)
return(coeff)
}
results <- boot(data=Xy, statistic=lm.fn, R=1000, formula=y~X1+X2)
results
new.rows = c(101:200, 401:500, 101:200, 901:1000, 301:400, 1:100, 1:100, 801:900, 201:300, 701:800)
new.Xy = Xy[new.rows, ]
results <- boot(data=new.Xy, statistic=lm.fn, R=1000, formula=y~X1+X2)
results
library(ISLR)
summary(Hitters)
points(10, reg.summary$cp[10], pch=20, col="red")
library(leaps)
regfit.full=regsubsets(Salary~., data=Hitters)
reg.summary=summary(regfit.full)
regfit.full=regsubsets(Salary~., data=Hitters, nvmax=19)
library(ISLR)
summary(Hitters)
```
There as some missing values here, so before we proceed we will remove them:
```{r, echo=FALSE}
Hitters = na.omit(Hitters)
with(Hitters, um(is.na(Salary)))
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
```{r}
library(leaps)
regfit.full=regsubsets(Salary~., data=Hitters)
reg.summary=summary(regfit.full)
regfit.full=regsubsets(Salary~., data=Hitters, nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], pch=20, col="red")
```
library(glmnet)
install.packages("glmnet")
cv.glmnet()
??cv.glmnet()
sd(c(5,8,12))
which.min(c(4,1,6))
library(readr)
library(ggmap)
install.packages(c("readr", "ggmap"))
View(new.Xy)
View(new.Xy)
View(new.Xy)
library(ggmap)
ggtitle("Distribution of Mosquito Counts (Log Scale")
library(readr)
library(ggmap)
data_dir <- "../input"
train <- read_csv(file.path(data_dir, "train.csv"))
mapdata <- readRDS(file.path(data_dir, "mapdata_copyright_openstreetmap_contributors.rds"))
train$Date <- as.Date(train$Date)
counts_by_date <- train %>% group_by(Date) %>% summarise(NumMeasurements = n()) %>% arrange(desc(NumMeasurements)) %>% head
counts_by_date
date_to_show <- counts_by_date$Date[1]
single_date_grouped_by_location <- train %>%
filter(Date == "2007-08-01") %>%
group_by(Longitude, Latitude) %>%
summarize(NumMosquitos = sum(NumMosquitos))
qplot(single_date_grouped_by_location$NumMosquitos) +
scale_x_log10() +
xlab("Number of Mosquitos") +
ylab("Number of test sites w/ this many mosquitos") +
ggtitle("Distribution of Mosquito Counts (Log Scale")
library(readr)
library(ggmap)
data_dir <- "../input"
train <- read_csv(file.path(data_dir, "train.csv"))
mapdata <- readRDS(file.path(data_dir, "mapdata_copyright_openstreetmap_contributors.rds"))
train$Date <- as.Date(train$Date)
counts_by_date <- train %>% group_by(Date) %>% summarise(NumMeasurements = n()) %>% arrange(desc(NumMeasurements)) %>% head
counts_by_date
date_to_show <- counts_by_date$Date[1]
library(readr)
library(ggmap)
data_dir <- "../input"
train <- read_csv(file.path(data_dir, "train.csv"))
mapdata <- readRDS(file.path(data_dir, "mapdata_copyright_openstreetmap_contributors.rds"))
train$Date <- as.Date(train$Date)
counts_by_date <- train %>% group_by(Date) %>% summarise(NumMeasurements = n()) %>% arrange(desc(NumMeasurements)) %>% head
counts_by_date
date_to_show <- counts_by_date$Date[1]
library(ggmap)
data_dir <- "../input"
train <- read_csv(file.path(data_dir, "train.csv"))
mapdata <- readRDS(file.path(data_dir, "mapdata_copyright_openstreetmap_contributors.rds"))
train$Date <- as.Date(train$Date)
counts_by_date <- train %>% group_by(Date) %>% summarise(NumMeasurements = n()) %>% arrange(desc(NumMeasurements)) %>% head
counts_by_date
date_to_show <- counts_by_date$Date[1]
single_date_grouped_by_location <- train %>%
filter(Date == "2007-08-01") %>%
group_by(Longitude, Latitude) %>%
summarize(NumMosquitos = sum(NumMosquitos))
qplot(single_date_grouped_by_location$NumMosquitos) +
scale_x_log10() +
xlab("Number of Mosquitos") +
ylab("Number of test sites w/ this many mosquitos") +
ggtitle("Distribution of Mosquito Counts (Log Scale")
ggmap(mapdata) +
geom_point(aes(x=Longitude, y=Latitude, color=NumMosquitos), size=3, data=single_date_grouped_by_location) +
scale_color_continuous(trans="log") +
citation()
getwd()
setwd("./Desktop/Analytics Edge/Final Exam/")
Movies <- read.csv("./Movies.csv")
rownames(Movies)
str(Movies)
MoviesTrain <- subset(Movies, Year<2010)
str(MoviesTrain)
MoviesTest <- subset(Movies, Year>=2010)
str(MoviesTest)
mod1 = glm(Worldwide~PropR, data=MoviesTrain[ , 3:ncol(MoviesTrain)])
mod1 = glm(Worldwide~., data=MoviesTrain[ , 3:ncol(MoviesTrain)])
ModelLR = glm(Worldwide~., data=MoviesTrain[ , 3:ncol(MoviesTrain)])
summary(ModelLR)
ModelLR = lm(Worldwide~., data=MoviesTrain[ , 3:ncol(MoviesTrain)])
summary(ModelLR)
cor(MoviesTrain)
MoviesTrain <- subset(Movies, Year<2010)
cor(MoviesTrain)
cor(MoviesTrain[ , 3:ncol(MoviesTrain)])
cor(MoviesTrain$Worldwide, MoviesTrain$Production.Budget)
ModelLR2 = lm(Worldwide~ Runtime + Crime + Horror + Animation + History + Nominations + Production.Budget, data=MoviesTrain[ , 3:ncol(MoviesTrain)])
summary(ModelLR2)
MoviesTrain$Worlwide
MoviesTest$Worlwide
ModelLR2.pred = predict(ModelLR2, newdata=MoviesTest)
ModelLR2.sse = sum((ModelLR2.pred - test$Worldwide)^2)
ModelLR2.sse = sum((ModelLR2.pred - MoviesTest$Worldwide)^2)
ModelLR2.sse
table(MoviesTest$Worlwide)
table(MoviesTest)
table(MoviesTest$Worlwide, ModelLR2.pred)
table(MoviesTest[ , 3:ncol(MoviesTrain)]$Worlwide, ModelLR2.pred)
ModelLR2.sst = sum((mean(MoviesTest$Worldwide) - MoviesTest$Worldwide)^2)
ModelLR2.sst
R2 = 1 - (ModelLR2.sse/ModelLR2.sst)
R2
summary(ModelLR2)
Movies$Performance = factor(ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .75), "Excellent", ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .25), "Average", "Poor")))
str(Movies$Performance)
summary(Movies$Performance)
Movies$Worldwide = NULL
library(caTools)
set.seed(15071)
spl = sample.split(Movies$Performance, SplitRatio = 0.7)
MoviesTrain = subset(Movies, spl==TRUE)
MoviesTest = subset(Movies, spl==FALSE)
library(rpart)
library(rpart.plot)
MoviesTree = rpart(Performance ~. , data=MoviesTrain[ , 3:ncol(MoviesTrain)], method="class")
prp(MoviesTree)
table(MoviesTrain$Performance)
table(MoviesTrain$Performance, MoviesTree)
PredictTrain = predict(MoviesTree, type="class")
table(MoviesTrain$Performance, PredictTrain)
(96+41+46)/nrow(MoviesTrain)
table(MoviesTrain$Performance)
116/(116+59+59)
PredictTest = predict(MoviesTree, newdata=MoviesTest, type="class")
table(MoviesTest$Performance, PredictTest)
(36+16+16)/nrow(MoviesTest)
table(MoviesTest$Performance)
fedFunds = read.csv("./federalFunds.csv", stringsAsFactors=FALSE")
fedFunds = read.csv("./federalFunds.csv", stringsAsFactors=FALSE")
fedFunds <- read.csv("./federalFunds.csv", stringsAsFactors=FALSE)
fedFunds <- read.csv("./federalFundsRate.csv", stringsAsFactors=FALSE)
summary(fedFunds$Streak)
str(fedFunds$Streak)
table(fedFunds$Streak)
table(fedFunds$Streak>0)
295/(290+295)
which.max(fedFunds$Chairman)
table(fedFunds$Chairman)\
table(fedFunds$Chairman)
max(fedFunds$Chairman)
fedFunds$Chairman = as.factor(fedFunds$Chairman)
fedFunds$DemocraticPres = as.factor(fedFunds$DemocraticPres)
fedFunds$RaisedFedFunds = as.factor(fedFunds$RaisedFedFunds)
set.seed(201)
library(caTools)
spl = sample.split(fedFunds$RaisedFedFunds, 0.7)
training = subset(fedFunds, spl==TRUE)
testing = subset(fedFunds, spl==FALSE)
Mod = glm(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemoncraticPres + MonthsUntilElection, data=training, family=binomial)
summary(Mod)
str(training)
Mod = glm(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data=training, family=binomial)
summary(Mod)
Mod = glm(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data=training, family=binomial)
summary(Mod)
summary(training)
summary(Mod)
summary(training)
prediction = 9
prediction = 9.121012 - 0.003427*1.7 + 0.157658*(-3) - 0.047449*5.1 - 0.136451*65.3 + 0.347829*0 - 0.006931*18
prediction
summary(Mod)
9.121012 - 0.003427*1.7 - 0.157658*3 - 0.047449*5.1 - 0.136451*65.3 + 0.347829*0 - 0.006931*18
training$Unemployment
training$HomeownershipRate
summary(Mod)
training$DemocraticPres
training$MonthsUntilElection
summary(Mod)
training$Streak
summary(Mod)
9.121012 - 0.003427*1.7 - 0.157658*3 - 0.047449*5.1 - 0.136451*65.3 + 0.347829*0 - 0.006931*18
glm(-0.6347861)
1+ -0.6347861
Mod(-0.6347861)
Mod
glm(RaisedFedFunds=-0.6347861)
log(0.6347861)
log(0.347829)
exp(0.347829)
PredTest = predict(Mod, newdata=testing, type="response")
summary(PredTest)
table(test$RaisedFedFunds, PredTest>0.5)
table(testing$RaisedFedFunds, PredTest>0.5)
(60+57)/nrow(testing)
str(testing)
str(training)
table(testing$RaisedFedFunds, PredTest>0.5)
(60+57)/nrow(testing)
table(training$RaisedFedFunds)
table(testing$RaisedFedFunds)
27+31
PredTest
PredTst>0.5
PredTest>0.5
table(PredTest)
summary(PredTest)
sum(PredTest)
count(PredTest)
str(PredTest)
str(PredTest>0.5)
table(PredTest>0.5)
install.packages("ROCR")
library(ROCR)
predROCR = prediction(PredTest, testing$RaisedFedFunds)
perfROCR = performance(predROCR, "tpr", "fpr")
performance(predROCR, "auc")@y.values
plot(perfROCR, colorize=TRUE)
set(201)
set.seed(201)
library(ggplot2)
library(lattice)
install.packages("caret")
library(caret)
numFolds = trainControl(method = "cv", number = 10)
cpGrid = expand.grid(.cp = seq(0.01,0.5,0.01))
numFolds = trainControl(method = "cv", number = 10) # 10-fold cross validation
cpGrid = expand.grid(.cp = seq(0.001,0.050,0.001)) # 50 values of +0.001 (0.001-0.05)
train(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = training, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
treecv = rpart(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data=training, cp=0.05)
prp(treecv)
treecv = rpart(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data=training, cp=0.05)
treecv = rpart(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data=training)
treecv
prp(treecv)
testing.pred = predict(treecv, newdata=testing, type="class")
testing.pred
table(testing$RaisedFedFunds)
table(testing$RaisedFedFunds, testing.pred>=0.5)
table(testing$RaisedFedFunds, testing.pred)
(70+46)/(70+17+42+46)
88/(88+87)
training.pred = predict(treecv)
training$RaisedFedFunds = as.factor(training$RaisedFedFunds)
table(training$RaisedFedFunds, training.pred)
table(training$RaisedFedFunds, training.pred>=0.5)
table(testing$RaisedFedFunds, testing.pred>=0.5)
treecv = rpart(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data=training, cp=0.05)
prp(treecv)
testing.pred = predict(treecv, newdata=testing, type="class")
testing.pred
table(testing$RaisedFedFunds, testing.pred>=0.5)
table(testing$RaisedFedFunds, testing.pred)
(79+36)/(79+36+8+52)
numFolds = trainControl(method = "cv", number = 10) # 10-fold cross validation
cpGrid = expand.grid(.cp = seq(0.001,0.050,0.001)) # 50 values of +0.001 (0.001-0.05)
train(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = training, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
treecv = rpart(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data=training, cp=0.23)
prp(treecv)
training.pred = predict(treecv)
training$RaisedFedFunds = as.factor(training$RaisedFedFunds)
testing.pred = predict(treecv, newdata=testing, type="class")
testing.pred
table(testing$RaisedFedFunds, testing.pred)
pred.prob = testing.pred[,2]
pred.prob = testing.pred
table(testing$RaisedFedFunds, testing.pred >=0.5)
table(testing$RaisedFedFunds, testing.pred >0.5)
table(testing$RaisedFedFunds, testing.pred[2]>0.5)
testing.pred[2]
testing.pred[,2]
testing.pred[,1]
Households = read.csv("./Households.csv")
str(Households)
str(Households$AfternoonPct)
table(Households$AfternoonPct)
table(Households$AfternoonPct, Households$MorningPct)
summary(Households$AfternoonPct)
table(Households)
summary(Households)
summary(Households$AfternoonPct)
table(Households$MorningPct>0)
table(Households$AfternoonPct>0)
head(Households)
table(Households$MorningPct, Households$AfternoonPct)
table(Households$MorningPct>0, Households$AfternoonPct>0)
table(Households$MorningPct>0, Households$AfternoonPct)
table(Households$MorningPct>0, Households$AfternoonPct>0)
table(Households$MorningPct>100, Households$AfternoonPct>0)
table(Households$MorningPct>0, Households$AfternoonPct>100)
table(Households$MorningPct>0)
table(Households$AfternoonPct>0)
table(Households$AfternoonPct>=100)
sub = subset(Households, Households$AvgSalesValue>150)
which.min(Households$AvgDiscount, sub)
which.min(sub$AvgDiscount)
which.max(sub$AvgDiscount)
sub
sub2 = subset(Households, Households$AvgDiscount>25)
which.min(sub2$AvgSalesValue)
sub$AvgDiscount[3]
sub$AvgDiscount[4]
sub2$AvgSalesValue[4]
sub3 = subset(Households, Households$NumVisits>=300)
sub3
str(sub3)
148/2500
sub2 = subset(Households, Households$AvgDiscount>25)
which.min(sub2$AvgSalesValue)
sub2$AvgSalesValue[1]
library(caret)
preproc = preProcess(Households)
HouseholdsNorm = predict(preproc, Households)
summary(HouseholdsNorm)
distances <- dist(HouseholdsNorm, method = "euclidean")
ClusterShoppers <- hclust(distances, method = "ward.D")
plot(ClusterShoppers, labels = FALSE)
set.seed(200)
k=10
KMC = kmeans(distances, centers = k, iter.max = 1000)
str(KMC)
householdsClusters = KMC$cluster
householdsClusters
KMC$centers[2]
householdsClusters
table(householdsClusters)
KMC$centers[2]
set.seed(200)
k=10
KMC = kmeans(distances, centers = k)
str(KMC)
householdsClusters = KMC$cluster
householdsClusters
table(householdsClusters)
table(KMC)
set.seed(200)
distances <- dist(HouseholdsNorm, method = "euclidean")
ClusterShoppers <- hclust(distances, method = "ward.D")
plot(ClusterShoppers, labels = FALSE)
k=10
KMC = kmeans(distances, centers = k, iter.max = 1000)
str(KMC)
householdsClusters = KMC$cluster
householdsClusters
table(householdsClusters)
k=10
set.seed(200)
kmeans = kmeans(distances, centers = k, iter.max = 1000)
kmeans$cluster
table(kmeans$cluster)
kmeansClust
kmeansClust = kmeans$cluster
kmeansClust
kmeans = kmeans(HouseholdsNorm, centers = k, iter.max = 1000)
kmeans$cluster
table(kmeans$cluster)
kmeans$cluster
table(kmeans$cluster)
kmeansClust = kmeans$cluster
kmeansClust
k=10
set.seed(200)
kmeans = kmeans(HouseholdsNorm, centers = k, iter.max = 1000)
kmeans$cluster
table(kmeans$cluster)
kmcluster1 = subset(HouseholdsNorm, kmeansClust==1)
kmcluster2 = subset(HouseholdsNorm, kmeansClust==2)
kmcluster3 = subset(HouseholdsNorm, kmeansClust==3)
kmcluster4 = subset(HouseholdsNorm, kmeansClust==4)
kmcluster5 = subset(HouseholdsNorm, kmeansClust==5)
kmcluster6 = subset(HouseholdsNorm, kmeansClust==6)
kmcluster7 = subset(HouseholdsNorm, kmeansClust==7)
kmcluster8 = subset(HouseholdsNorm, kmeansClust==8)
kmcluster9 = subset(HouseholdsNorm, kmeansClust==9)
kmcluster10 = subset(HouseholdsNorm, kmeansClust==10)
str(kmcluster1)
summary(kmcluster1)
table(kmcluster1)
summary(kmcluster2)
summary(kmcluster1)
table(kmeansClust$AvgProdCount, mean)
table(HouseholdsNorm$AvgProdCount, mean)
summary(kmcluster3)
summary(kmcluster1)
summary(kmcluster2)
summary(kmcluster3)
summary(kmcluster4)
summary(kmcluster5)
summary(kmcluster6)
summary(kmcluster7)
summary(kmcluster8)
summary(kmcluster9)
summary(kmcluster10)
tapply(Households$Balance, kmeansClust, mean)
clusterGroups = cutree(HouseholdsNorm, k = 10)
clusterGroups = cutree(Households, k = 10)
tapply(HouseholdsNorm$Balance, clusterGroups, mean)
tapply(HouseholdsNorm$Balance, kmeansClust, mean)
table(kmeans$cluster)
tapply(Households$Balance, kmeans$cluster, mean)
tapply(HouseholdsNorm$Balance, kmeans$cluster, mean)
tapply(Households$AvgProdCount, kmeansClust, mean)
tapply(Households$NumVisits, kmeansClust, mean)
str(Households)
tapply(Households$NumVisits, kmeansClust, mean)
tapply(Households$AvgProdCount, kmeansClust, mean)
tapply(Households$AvgDiscount, kmeansClust, mean)
tapply(Households$AvgSalesValue, kmeansClust, mean)
tapply(Households$MorningPct, kmeansClust, mean)
tapply(Households$AfternoonPct, kmeansClust, mean)
k=5
set.seed(200)
kmeans = kmeans(HouseholdsNorm, centers = k, iter.max = 1000)
kmeans$cluster
table(kmeans$cluster)
k=5
set.seed(500)
kmeans = kmeans(HouseholdsNorm, centers = k, iter.max = 1000)
kmeans$cluster
table(kmeans$cluster)
kmeansClust = kmeans$cluster
kmeansClust
tapply(HouseholdsNorm$Balance, kmeans$cluster, mean)
tapply(Households$NumVisits, kmeansClust, mean)
tapply(Households$AvgProdCount, kmeansClust, mean)
tapply(Households$AvgDiscount, kmeansClust, mean)
tapply(Households$AvgSalesValue, kmeansClust, mean)
tapply(Households$MorningPct, kmeansClust, mean)
